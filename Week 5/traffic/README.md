For my first attempt to create a compiled neural network model, I used the same values that are used in the lecture's source code example. The example involves training a neural network with MNIST so that the model can recognize different handwritten numbers. I thought that this is quite similar to a model that can recognize different traffic signs because both involves reading established, universal symbols. I used the same number of convolutional layers, kernel size, pool size, hidden layer, and dropout. This felt like a good place to start. Turns out, the accuracy was quite terrible (less than 50%). I then went back to my code and asked myself, do these values I've set make sense in the context of traffic sign recognition? I searched the internet on how to determine the number of filters to use and what size the kernel should be. I learned that 3x3 kernel is large enough, since every traffic image is already resized to 30x30. So I kept the kernel size as 3x3. The number of filters had to be changed, however. Traffic signs are a lot more complex than numbers, so I increased the number of filters 128. I ran it again, the accuracy improved but it took longer to run. I then tried increasing the size of the hidden layer from 128 to 256, which made a significant improvement in accuracy, although it did take longer to build the model. I then changed it to 512, which took too long to run, so I changed it back to 256. I then searched the internet on how to determine the dropout value, and from what I've learned, 0.5 seems to be the best value to use. 

I adjusted the values againa and again, but the accuracy never reached above 95% without taking forever to run. I felt like something was missing. Then I realized that I only have 1 convolutional layer, and that adding a second one would help a lot. I added another one, and since I have 2 concolutional layers, I could lower the number of filters for each layer so it could run faster. The first one had 64, and the second had 128. I ran it again, and the accuracy went up to about 97%, and each Epoch only took 15 seconds. Then I tried lowering the number of filters for the convolutional layers to 32 and 64, thinking that it might be even better. Turns out, the accuracy was about the same, but it did run a lot faster. I changed it back to 128 and 64. I then decided to adjust the hidden layer, so I decrease the number from 256 to 128 to see what would happen. The accuracy didn't change much, but I still kept it at that value. I then changed the number of filters in the two convolutional layers to 32 and 64. The accuracy went up to 96%, which I think is good enough. It also runs very fast.